\section{Deep Learning}

In the field of machine learning, the first models you
are often introduced to are those for regression
and classification that utilize linear
combinations of fixed basis functions.
According to \cite{Bishop:2008aa},
these models possess useful analytical and computational properties,
but their practical applicability is constrained 
since their capacity is limited to linear functions
and they cannot understand the interaction between any two input variables.
This leads to problems such as the \textbf{curse of dimensionality},
where the number of possible interactions between variables grows exponentially
with the number of input variables.
To address large-scale problems, it is essential to adapt the basis functions
to the data, as demonstrated by support vector machines (SVMs).
Alternatively, one can fix the number of basis functions in advance while
allowing them to be adaptive, utilizing parametric forms for the basis
functions with parameter values adjusted during training.

The most successful example of this approach in pattern recognition is the
\textbf{feed-forward neural network},
also known as the multilayer perceptron (MLP).
\begin{figure}[h]
    \centering
    \includegraphics[width=.75\textwidth]{figures/ch3/1.mlp.png}
    \caption{A feed-forward neural network with two hidden layers}
    \vspace{-10px}
    \caption*{\scriptsize{Source: \href{https://brilliant.org/wiki/feedforward-neural-networks/}{Brilliant}}}
    \label{fig:mlp}
\end{figure}

As explained in 
\cite{goodfellow2016deep},
the goal of a feedforward network is to approximate a specific function
\( f^* \). For example, in a classification task,
\( y = f^*(x) \) maps an input \( x \) to a category \( y \).
A feedforward network defines a mapping \( y = f(x; \theta) \) and
learns the parameters \( \theta \) that yield the best approximation
of this function.

These models are called \textbf{feedforward} because information
flows unidirectionally from the input \( x \), through intermediate computations
defining \( f \), and ultimately to the output \( y \), without feedback loops.

The training data directly specifies the output layer's behavior but
not the intermediate layers, which are called \textbf{hidden layers}
because their desired outputs are not provided by the training data.
The learning algorithm must determine how to best utilize these
hidden layers to approximate \( f^* \).
Every layer of the network computes a non-linear transformation of the
previous layer's activations, this way a complex function can be
approximated by composing simpler functions, one for each layer.
Layers are composed of a set of \textbf{units}, where each unit is a node that
computes a non-linear function of the weighted sum of its inputs
and is only connected to units in the previous and the following layer.

In order to train the network and update the weights, MLPs use the
\textbf{backpropagation} technique, which computes the gradient of the
loss function with respect to the weights of the network.
The weights are then updated using an optimization algorithm such as
\textbf{stochastic gradient descent} (SGD)
or one of its adaptive variants like \textbf{Adam},
whose hyperparameters are tuned according to the task during training.

A feedforward network is called a \textbf{deep neural network} if it has
more than one hidden layer, the branch of machine learning that studies
deep neural networks is called \textbf{deep learning}.

They are called networks due to their structure, which involves composing
multiple functions together. Typically, this composition is represented
by a directed acyclic graph. For instance, functions
\( f^{(1)} \), \( f^{(2)} \), and \( f^{(3)} \) might be connected in
a chain to form \( f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x))) \).

Functions \( f^{(1)} \) and \( f^{(2)} \) must
be non linear, otherwise the composition would collapse into a single
linear function. These non-linear functions are
called \textbf{activation functions}.
The last function \( f^{(3)} \) is typically a linear function that maps
the output of the final hidden layer to the output layer.
This is the same as applying a linear model to a transformed
input \( \phi(x) \), where \( \phi \) is a nonlinear transformation.
The question then becomes how to choose the mapping \( \phi \).

The strategy of deep learning is to learn \( \phi \):
in this approach, we use a model
\begin{equation}
y = f(x; \theta, w) = \phi(x; \theta)^\top w
\end{equation}
Here, we have parameters \( \theta \) that are used to learn \( \phi \)
from a broad class of functions, and parameters \( w \) that map \( \phi(x) \)
to the desired output.
This approach allows for greater flexibility:
specifically, by using a broad family of functions
\( \phi(x; \theta) \), the human designer only needs to select the appropriate
general function family rather than finding precisely an exact function.

The \textbf{universal approximation theorem} \cite{HORNIK1989359}
tells us that regardless of what function
we are trying to learn, we know that a feedforward network with enough units
will be able to \emph{represent} this
function, however we are not guaranteed that the training algorithm will be able
to \emph{learn} it.

\section{Reinformcement Learning}

``\textbf{Reinforcement learning} is learning [...] how
to map situations to actions so
as to maximize a numerical reward signal.
The learner is not told which actions to
take, but instead must discover which actions
yield the most reward by trying them. In
the most interesting and challenging cases, actions may
affect not only the immediate
reward but also the next situation and,
through that, all subsequent rewards.
These two
characteristics, \textbf{trial-and-error search} and \textbf{delayed reward},
are the two most important
distinguishing features of reinforcement learning.'' \cite{sutton1998}

Let's explore the basic concepts of reinforcement learning with
a simple example as described in \cite{zhao2024RLBook}.
Consider a grid world scenario as shown in Figure \ref{fig:rl}
where a robot, referred to as
the \textbf{agent}, moves between cells, occupying one cell at a time.
The white cells are accessible, while the orange cells are forbidden.
The goal is for the agent to reach a target cell.

\begin{figure}[h]
    \centering
    \includegraphics[width=.42\textwidth]{figures/ch3/2.rl.png}
    \caption{A simple reinforcement learning task}
    \vspace{-10px}
    \caption*{\scriptsize{Source: \cite{zhao2024RLBook}}}
    \label{fig:rl}
\end{figure}

To achieve this, the agent needs a \textbf{policy} that guides it to the target
efficiently, avoiding forbidden cells and unnecessary detours.
If the grid layout is known, planning is simple.
However, without prior knowledge, the agent must explore and learn
through trial and error.

The agent's status in the grid is defined by its \textbf{state}
\( s_i \in \mathcal{S} \), which represents its location relative
to the \textbf{environment}.
In the examples with nine cells, the state space is
\( \mathcal{S} = \{s_1, s_2, \ldots, s_9\} \)..

From each state, the agent can perform five \textbf{actions}: move up,
right, down, left or stay put, denoted as \( a_1, a_2, \ldots, a_5 \),
this set of actions is the action space
\( \mathcal{A} = \{a_1, \ldots, a_5\} \).
The available actions can vary by state, for instance,
at \( s_1 \), actions \( a_1 \) (up) and \( a_4 \) (left) would collide
with the grid boundary, so the action space is
\( \mathcal{A}(s_1) = \{a_2, a_3, a_5\} \).

When taking an action, the agent may move from one state to another,
a process known as \textbf{state transition}. For example,
if the agent is at state \( s_1 \) and selects action
\( a_2 \) (moving rightward), it transitions to state
\( s_2 \). This process can be represented as:
\[ s_1 \overset{a_2}{\longrightarrow} s_2 \]
The state transition process is defined for each
state and its associated actions. Mathematically, state transitions
are described by conditional probabilities.
For example, for \( s_1 \) and \( a_2 \),
the conditional probability distribution is:
\[
\begin{aligned}
p(s_1 \mid s_1, a_2) &= 0, \\
p(s_2 \mid s_1, a_2) &= 1, \\
p(s_3 \mid s_1, a_2) &= 0, \\
p(s_4 \mid s_1, a_2) &= 0, \\
p(s_5 \mid s_1, a_2) &= 0,
\end{aligned}
\]
indicating that taking \( a_2 \) at \( s_1 \) guarantees
the agent moves to \( s_2 \), with a probability of one,
and zero probability for other states.

This is a deterministic state transition,
but state transitions can also be stochastic,
requiring conditional probability distributions.
For instance, if random wind gusts affect the grid,
taking action \( a_2 \) at \( s_1 \) might blow the agent to \( s_5 \)
instead of \( s_2 \), resulting in \( p(s_5 \mid s_1, a_2) > 0 \).

A \textbf{policy} is a function that maps states to actions,
indicating the agent's behavior in the environment.
In other words, a policy tells the agent what action to take
at each state.

Mathematically, policies can be described by conditional probabilities.
For example, the policy for \( s_1 \) is:
\[
\begin{aligned}
\pi(a_1 \mid s_1) &= 0, \\
\pi(a_2 \mid s_1) &= 1, \\
\pi(a_3 \mid s_1) &= 0, \\
\pi(a_4 \mid s_1) &= 0, \\
\pi(a_5 \mid s_1) &= 0,
\end{aligned}
\]
indicating that the probability of taking action \( a_2 \) at state \( s_1 \)
is one, and the probabilities of taking other actions are zero.

The above policy is deterministic, but policies may generally be stochastic.
For instance, let's assume that at state \( s_1 \)
the agent may take actions to move either rightward or downward,
each with a probability of 0.5. In this case, the policy for \( s_1 \) is:
\[
\begin{aligned}
\pi(a_1 \mid s_1) &= 0, \\
\pi(a_2 \mid s_1) &= 0.5, \\
\pi(a_3 \mid s_1) &= 0.5, \\
\pi(a_4 \mid s_1) &= 0, \\
\pi(a_5 \mid s_1) &= 0.
\end{aligned}
\]

After executing an action at a state, the agent receives a
reward \( r \) as feedback from the environment.
The \textbf{reward} is a function of the state and action
which predicts immediate reward and is denoted as:
\begin{equation}
R(s_t=s, a_t=a) = \mathbb{E} [r_t | s_t = s, a_t = a]
\end{equation}
and it can be positive, negative, or zero.
Different rewards influence the policy the agent learns.
Generally, a positive reward encourages the agent to take the
corresponding action, while a negative reward discourages it.

However we can't find good policies
by simply selecting actions with the greatest immediate rewards
since they do not consider
long-term outcomes. To determine a good policy, we must consider the total
reward obtained in the long run and
an action with the highest immediate reward may not lead to
the greatest total reward.

A \textbf{trajectory} is a state-action-reward chain.
For example, the agent in our example may follow this trajectory:
\[ s_1 \xrightarrow{a_2, r=0} s_2 \xrightarrow{a_3, r=0} s_5 \xrightarrow{a_3, r=0} s_8 \xrightarrow{a_2, r=1} s_9. \]
The \textbf{return} of this trajectory is the sum of all rewards collected along it,
in the example above:
\[\text{return} = 0+0+0+1=1\]
Returns, also called total rewards or cumulative rewards,
are used to evaluate policies.
Returns can also be defined for infinitely long trajectories, which may diverge.
Therefore, we introduce the concept of \textbf{discounted return} for infinitely
long trajectories. The discounted return is the sum of the rewards from $t$ to $T$ (final step):
\begin{equation}
G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{equation}
where \( \gamma \in [0, 1] \) is the \textbf{discount factor}.
If \( \gamma \) is close to 0, the agent emphasizes near-future rewards,
resulting in a short-sighted policy. If \( \gamma \) is close to 1,
the agent emphasizes far-future rewards.

When $T$ is finite, we call the task episodic and each sequence
up to the \textbf{terminal state} is an \textbf{episode}. Otherwise, we refer
to \textbf{continuing tasks}.

The \textbf{Markov Decision Processes} (MDPs),
a general framework for describing stochastic dynamical systems,
allows us to formally presents these concepts.
The \textbf{Markov property} refers to the \textbf{memoryless} property of a stochastic process. Mathematically, it means that
\begin{equation}
\begin{aligned}
p(s_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_0, a_0) &= p(s_{t+1} \mid s_t, a_t), \\
p(r_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_0, a_0) &= p(r_{t+1} \mid s_t, a_t),
\end{aligned}
\end{equation}
where \( t \) represents the current time step and \( t + 1 \) represents the next time step.
This indicates that the next state or reward depends only on the current
state and action and is independent of the previous ones.

\vspace{3cm}

An MDP is defined by a tuple \( \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle \),
let us break down the components:
\begin{itemize}
    \item \( \mathcal{S} \): finite set of Markov states $s$
    \item \( \mathcal{A} \): finite set of actions $a$
    \item \( P \): state transition model for each action,
    a probability matrix that specifies $$p(s_t+1 = s'|s_t=s, a_t=a)$$
    \item \( R \): reward function
    $$ R(s_t=s, a_t=a) = \mathbb{E} [r_t | s_t = s, a_t = a] $$
    \item \( \gamma \): discount factor $0 \leq \gamma \leq 1$
\end{itemize}

When the policy in a MDP is fixed, it reduces to a \textbf{Markov Process} (MP),
this transformation simplifies the MDP by eliminating the decision-making aspect. 
A Markov process is referred to as a \textbf{Markov Chain} if it operates in discrete
time and the number of states is either finite or countable.

\begin{figure}[h]
    \centering
    \includegraphics[width=.75\textwidth]{figures/ch3/3.mdp.png}
    \caption{The example grid as a Markov Process graph, where the nodes
    represent the states and the edges represent the state transitions.}
    \vspace{-10px}
    \caption*{\scriptsize{Source: \cite{zhao2024RLBook}}}
    \label{fig:mdp}
\end{figure}


\subsection{Reinforcement Learning Algorithms}

In \cite{openaiPartKinds}, the landscape of algorithms in modern reinforcement
learning is explored.

The algorithms used in reinforcement learning almost always rely on
\textbf{value functions}.
The \textbf{value} is the expected return if the agent starts in
that state or state-action pair and then follows a specific policy indefinitely.

\vspace{5cm}

There are four primary value functions to consider:

\begin{itemize}
    \item \textbf{On-Policy Value Function, \( V^{\pi}(s) \)}: This function
    represents the expected return if the agent starts in state \( s \)
    and acts according to the policy \( \pi \):
    \begin{equation}
    V^{\pi}(s) = \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) \mid s_0 = s \right]
    \end{equation}
    \item \textbf{On-Policy Action-Value Function, \( Q^{\pi}(s,a) \)}:
    This function gives the expected return if the agent starts in state \( s \),
    takes an arbitrary action \( a \), and then always follows the policy \( \pi \):
    \begin{equation}
    Q^{\pi}(s,a) = \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) \mid s_0 = s, a_0 = a \right]
    \end{equation}
    \item \textbf{Optimal Value Function, \( V^*(s) \):} This function provides
    the expected return if the agent starts in state \( s \) and follows the
    optimal policy for the environment:
    \begin{equation}
    V^*(s) = \max_{\pi} \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) \mid s_0 = s \right]
    \end{equation}
    \item \textbf{Optimal Action-Value Function, \( Q^*(s,a) \):} This function
    represents the expected return if the agent starts in state \( s \),
    takes an arbitrary action \( a \), and then follows the optimal policy for the environment:
    \begin{equation}
    Q^*(s,a) = \max_{\pi} \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) \mid s_0 = s, a_0 = a \right]
    \end{equation}
\end{itemize}

Figure \ref{fig:mdp}
presents a partial taxonomy of the modern reinforcement learning algorithms.
One key distinction is whether the agent utilizes (or learns) a model of
the environment. A model, in this context, refers to a function that predicts
state transitions and rewards.

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{figures/ch3/4.rlalg.png}
    \caption{Partial taxonomy of algorithms in modern RL.}
    \vspace{-10px}
    \caption*{\scriptsize{Source: \cite{openaiPartKinds}}}
    \label{fig:mdp}
\end{figure}

The primary advantage of having a model is that it enables the agent
to plan by predicting future states and rewards.
However, the major drawback is that an accurate model of the environment
is often unavailable to the agent.
Algorithms which use a model are known as \textbf{model-based} methods,
while those that do not are referred to as \textbf{model-free} methods.
Our focus will be on the latter.

Model-free methods can be categorized into two main approaches
for representing and training agents:
\begin{itemize}
    \item \textbf{Policy Optimization:} explicitly represents a policy
    as \( \pi_{\theta}(a \mid s) \) and optimizes the parameters \( \theta \)
    either directly via gradient ascent on the performance objective
    \( J(\pi_{\theta}) \) or indirectly by maximizing local approximations
    of \( J(\pi_{\theta}) \).
    Typically, this optimization is performed \textbf{on-policy},
    meaning that each update uses data collected while the agent is acting
    according to the most recent version of the policy.

    \item \textbf{Q-Learning:} involves learning an approximator
    \( Q_{\theta}(s,a) \) for the optimal action-value function \( Q^*(s,a) \).
    Unlike policy optimization, Q-learning is generally performed \textbf{off-policy},
    allowing updates to use data collected at any time during training,
    irrespective of the agent's policy at the time of data collection.
    The corresponding policy is derived from the relationship between
    \( Q^* \) and \( \pi^* \), where the actions taken by the Q-learning
    agent are determined by
    \begin{equation}
    a(s) = \arg \max_a Q_{\theta}(s,a)
    \end{equation}
    An example of a Q-learning algorithm is Deep Q-Network (\textbf{DQN}),
    which uses a deep neural network to approximate the optimal action-value
    function in environments with large state spaces,
    and its variants such as Double DQN (\textbf{DDQN}) and
    Dueling Double DQN (\textbf{D3QN}).

\end{itemize}

There are also hybrid approaches that combine elements
of both policy optimization and Q-learning.
In this spectrum of algorithms we can find 
\textbf{actor-critic methods}, which consist of two components:
an \textbf{actor} that makes actions and a \textbf{critic} that evaluates them.
An example of an actor-critic algorithm is Twin-Delayed DDPG (\textbf{TD3}).

If samples are collected during training the reinforcement learning is
considered \textbf{online}, otherwise,
if the training set is fixed, it is \textbf{offline}.

\section{Generative Adversarial Networks (GANs)}

\textbf{Generative Adversarial Networks} (GANs) is a framework
for estimating generative models via an adversarial process,
initially introduced by \cite{goodfellow2014}.
This framework involves training two models:
a \textbf{generative} model \( G \), which captures the data
distribution, and a \textbf{discriminative} model \( D \),
which predicts samples as either originating from the true
data distribution or the generative model.
The training objective for \( G \) is to maximize
the likelihood of \( D \) making incorrect classifications.
This adversarial process can be conceptualized as a minimax two-player game.
In the theoretical space of arbitrary functions \( G \) and \( D \),
a unique solution exists where \( G \) accurately
recovers the training data distribution and \( D \) outputs $\frac{1}{2}$
for all inputs (in other words, it reaches maximum uncertainty).
When \( G \) and \( D \) are defined by multilayer perceptrons,
the entire system can be trained using backpropagation.

To learn the generator's distribution \( p_g \)
over data \( x \), we define a prior \( p_z(z) \) on input noise variables
$z \in \Omega_Z$, called \textbf{latent distribution},
and represent a mapping to the data space as
\( G(z; \theta_g) \), where \( G: \Omega_Z \rightarrow \Omega_X \)
is a differentiable function parameterized by \( \theta_g \).
Additionally, we define a second network
\( D(x; \theta_d) \) that outputs a single scalar,
representing the probability that \( x \) originated
from the true data distribution $p_x$ rather than from \( p_g \).
The discriminative model \( D \) is trained to maximize
the probability of correctly labeling both the training
examples and the samples generated by \( G \).
Simultaneously, the generative model \( G \) is trained to minimize
\( \log(1 - D(G(z))) \).

Formally, \( D \) and \( G \) engage in the following
two-player minimax game with the value function \( V(G, D) \):
\begin{equation}
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_X(x)}[\log D(x)] + \mathbb{E}_{z \sim p_Z(z)}[\log (1 - D(G(z)))].
\end{equation}

We can see a graphical representation of the GAN framework in Figure \ref{fig:gan}.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/ch3/5.gan.png}
    \caption{GAN framework applied to human faces generation task}
    \vspace{-10px}
    \caption*{\scriptsize{Source: \href{https://www.linkedin.com/pulse/exploring-fascinating-realm-generative-adversarial-networks-kaurav/}{Yashwant Singh Kaurav}}}
    \label{fig:gan}
\end{figure}

\subsection{Bidirectional GANs (BiGANs)}

Generative Adversarial Networks (GANs) have the potential to be used
for unsupervised learning of rich feature representations across various
data distributions. However, an obvious issue arises because
the GAN framework inherently lacks an inverse mapping from generated data
back to the latent representation.
This limitation means that while the generator can map latent samples
to generated data, there is no mechanism to map data back to the latent space.

To address this issue, \cite{donahue2017} introduced a novel framework
called Bidirectional Generative Adversarial Networks (BiGAN).
The BiGAN framework, whose architecture is illustrated
in Figure \ref{fig:bigan}, enhances the standard GAN model
(as introduced by \cite{goodfellow2014}) by incorporating an \textbf{encoder}.
This encoder, denoted as \( E: \Omega_X \rightarrow \Omega_Z \),
maps data \( x \) to latent representations
\( z \), complementing the generator \( G \).

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/ch3/6.bigan.png}
    \caption{BiGAN architecture with generator, discriminator, and encoder.}
    \vspace{-10px}
    \caption*{\scriptsize{Source: \cite{donahue2017}}}
    \label{fig:bigan}
\end{figure}

In BiGAN, the discriminator \( D \) operates not only in the data space
(distinguishing between \( x \) and \( G(z) \)) but also jointly in the
data and latent spaces. Specifically, the discriminator evaluates pairs
of data and their corresponding latent representations, distinguishing
between real \( (x, E(x)) \) and generated \( (G(z), z) \).
Here, the latent component is either an encoder output \( E(x) \) 
or a generator input \( z \).

An essential aspect of BiGAN is that the encoder \( E \) is designed
to learn an inverse mapping of the generator \( G \).
Despite the encoder and generator not directly interacting, since
\( E(G(z)) \) is not explicitly computed and the generator does not use
\( E(x) \), the framework ensures that the encoder effectively inverts
the generator's mapping.

The training objective for BiGANs is formulated as a minimax problem involving the generator \( G \), the encoder \( E \), and the discriminator \( D \). This objective can be written as:
\[
\min_{G, E} \max_{D} V(D, E, G)
\]
where the value function \( V(D, E, G) \) is defined as:
\begin{equation}
V(D, E, G) := \mathbb{E}_{x \sim p_X} [ \underset{\log D(x,\, E(x)) }{\underbrace{\mathbb{E}_{z \sim p_E(\cdot | x)} \left[ \log D(x, z) \right]}}  ] +
\mathbb{E}_{z \sim p_Z} [ \underset{\log(1 - D(G(z),\, z))}{\underbrace{\mathbb{E}_{x \sim p_G(\cdot | z)} \left[ \log(1 - D(x, z)) \right]}} ].
\end{equation}
In simpler terms, this objective consists of two components:
\begin{enumerate}
    \item The expectation over the data distribution \( p_X \) and the encoder's latent space distribution \( p_E(\cdot | x) \), which aims to maximize \( \log D(x, E(x)) \).
    \item The expectation over the prior distribution \( p_Z \) and the generator's data distribution \( p_G(\cdot | z) \), which aims to maximize \( \log(1 - D(G(z), z)) \).
\end{enumerate}

The optimization of this minimax objective is performed using an alternating gradient-based approach, similar to the method introduced by \cite{goodfellow2014} for GANs.

\subsection{Bidirectional Conditional GANs (BiCoGANs)}

Conditional GAN (cGAN) (\cite{mirza2014})
is a variant of standard GANs designed to enable the
conditional generation of data samples based on both latent variables
(intrinsic factors) and known auxiliary information (extrinsic factors)
such as class labels or associated data from other modalities.
However, cGANs fails to achieve several key properties:

\begin{enumerate}
    \item The ability to disentangle intrinsic and extrinsic
    factors during the generation process.
    \item The ability to separate the components of extrinsic
    factors from each other, ensuring that the inclusion of one
    factor minimally impacts the others.
\end{enumerate}

\cite{jaiswal2018} introduced the Bidirectional Conditional GAN (BiCoGAN).
BiCoGAN enhances the cGAN framework by simultaneously training an encoder
along with the generator and discriminator.
This encoder learns inverse mappings of data samples to both intrinsic
and extrinsic factors, thereby overcoming deficiencies in prior approaches.
In Figure \ref{fig:bicogan}, we present the architecture of BiCoGAN.

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{figures/ch3/7.bicogan.jpeg}
    \caption{BiCoGAN architecture with generator, discriminator, and encoder.}
    \vspace{-10px}
    \caption*{\scriptsize{Source: \cite{jaiswal2018}}}
    \label{fig:bicogan}
\end{figure}

In the Bidirectional Conditional GAN (BiCoGAN) framework, the generator
\( G(\tilde{z}; \theta_G) \) learns a mapping from the distribution
\( p_{\tilde{z}} \), where \( \tilde{z} = [z, c] \), to \( p_G \),
with the goal of making \( p_G \) approximate \( p_X \). Concurrently,
the encoder \( E(x; \theta_E) \) learns a mapping from \( p_X \) to \( p_E \),
aiming to make \( p_E \) approximate \( p_{\tilde{z}} \). The discriminator
\( D \) evaluates real or fake decisions using pairs
\( (\tilde{z}, G(\tilde{z}); \theta_D) \) and \( (E(x), x; \theta_D) \).

The encoder in BiCoGAN must effectively learn the inverse mapping from data
\( x \) to latent variables \( z \) and conditions \( c \),
just as the generator must incorporate both to produce data samples
that can deceive the discriminator. This requirement follows from the
invertibility under the optimality theorem of BiGANs.
However, achieving this optimality is challenging in practice,
especially when the prior vector contains structured or complex information.
While intrinsic factors \( z \) are sampled from a simple latent distribution,
extrinsic factors \( c \), such as class labels or object attributes,
have specialized and complex distributions that are more difficult to model.

To address this challenge, we introduce the
\textbf{extrinsic factor loss} (EFL) as a mechanism to guide BiCoGANs
in better encoding extrinsic factors. During training, the condition
\( c \) associated with each real data sample is known and can be used to
improve the learning of inverse mappings from \( x \) to \( c \).
The specific form of EFL depends on the nature of \( c \) and the dataset
or domain in question.

The BiCoGAN value function can be expressed as:
\begin{equation}
\begin{aligned}
V(D, G, E) &:= \mathbb{E}_{x \sim p_X(x)} [\log D(E(x), x)]\\
&\quad + \gamma \, \mathbb{E}_{(x, c) \sim p_X(x, c)} [\text{EFL}(c, E_c(x))] \\
&\quad +\mathbb{E}_{z \sim p_{\tilde{z}} (\tilde{z})} [\log (1 - D(\tilde{z}, G(\tilde{z})))]
\end{aligned}
\end{equation}
where \( \gamma \) is the \textbf{extrinsic factor loss weight} (EFLW), defined as:
\[
\gamma = \min(\alpha e^{\rho t}, \phi)
\]
Here, \( \alpha \) is the initial value of \( \gamma \), \( \phi \) is its maximum value, \( \rho \) controls the rate of exponential increase, and \( t \) indicates the number of epochs the model has been trained.

\subsection{Wasserstein GANs (WGANs)}

\section{Causal Inference}

\cite{Neal_2020a}
